# Original Requirements - David Johnson
## Captured: February 15, 2025 (Signal voice call transcript + follow-up messages)

> **Note**: This transcript was captured via speech-to-text from a Signal video call between David Johnson and Paul Roth. Some words may be mistranscribed. The term "least competent nominator" in the transcript is believed to be a mistranscription of "least common denominator" (LCD), though David may have intended a different term - see LCD note at bottom.

---

## Core Concept: Evidence-Based Process Discovery

What I need is kind of like an HTML output from you to address - look at Camunda as like what would come out of Celonis and Sirocco, okay? And then you go down to business process models.

The point behind all this is, we're going to ingest a body of evidence. We're gonna do a shelf data request. We're going to ask clients for what they have.

### Evidence Sources (12 Categories)

1. **SaaS/Tool Exports** - If we have a relationship with Celonis, we could set up a monitoring agent to look at logs, task mining, all that level of detail
2. **BPM Process Models** - Organizations have BPM - diagrams, ARIS, Visio, all these different ways they've demonstrated their organizational definition
3. **Regulations/Policies/Controls** - The stuff that's between all the different items that drive people in their processes
4. **Controls and Evidence** - Audit logs, monitoring, that type of information
5. **Domain Communications** - Email archives, chat, transcripts, tickets, incident reports
6. **Job Aids/Edge Cases** - Decision trees, job aids, stuff like that

### The LCD Process

We take all of that in related to an area of the business. And we run it through the process to ingest the data, and now provide a first pass point of view in terms of what that process looks like.

We're not going to ask for everything cross-enterprise. We'll be focused. So we get a body of evidence related to a particular area. And then we produce our point of view on it. That's like the first pass.

I need something that would demonstrate through some sort of a flow, like one of those HTML flows, to demonstrate:
- How you take this body of evidence
- How you create the consistent, somewhat like semantic relationships between them
- So you could actually establish a point of view

Behind that point of view, you would have all the evidence based upon what was pulled. You'd be able to say, "this is our perspective of this. Here are the evidence items that we have associated with that."

**Maybe there's a confidence reading** based upon what we have and what we don't have relative to what that process might look like.

> "You're essentially kind of creating a **least competent nominator** [likely: least common denominator] point of view across the body of evidence, and then you're aligning the evidence that was uploaded to you, so that you could kind of have that point of view."

### Phase 1: Shelf Data Request & Initial POV

That's kind of like the first thing that we would do as we would be harvesting information from our clients through that shelf data request.

### Phase 2: TOM Alignment & Gap Analysis

Then we would take our target operating models, and we would align our target operating models. We would look at best practices out in the industry. We have a series of benchmarking functions that we look at as well, aside from our TOMs and best practices.

So then what we would do is essentially establish a point of view that says:
- "Here's what the evidence tells us for where you are"
- "Here are the other points of view that we need to take into consideration"

### Phase 3: Client Engagement

That's essentially the beginning of the conversations that we have with the client. We go through this process, we show up with that, and we say:
- "All right, this is what we have. It's all on the table."
- "We have a lot more to learn."
- "Now we're going to engage with you."
- "We're going to start to close the gap on these things."

That could lead to - that drives assessments and all the other things that we do to eliminate the ambiguity between what we have and where they might want to go.

---

## Extended Vision: SDLC Transformation

> From David's written paper (Friday night, referenced in call):

The model is: organizations need to create a factory around this concept of harvesting decision knowledge and process knowledge.

We need to do that for all the knowledge and intelligence that exist in the company. Like, we need a factory that just goes out in the organization and pulls it in.

### Three Context Layers

1. **Knowledge Context** - Decision knowledge and process knowledge from across the organization
2. **Regulatory/Policy Context** - Regulations, policies, controls, IT standards
3. **IT Strategy Context** - Engineering standards, IT strategies

### The Intersection

The software development lifecycle platform is the intersection point between:
- Your next idea and your next requirement
- Linking the knowledge that you have
- With the regulations, policies, and procedures
- With your IT strategies

To basically accelerate the development of those ideas.

> "The software development life cycle isn't just the thing that creates components. It's actually the thing that..."

*(Recording cuts off)*

---

## Follow-up Request (Feb 17, 2025)

David provided:
- A client-provided loan origination process diagram ("what they thought their current process was")
- A body of synthetic evidence
- Request to produce:
  1. Process based on the body of evidence (evidence-derived)
  2. Best practice for Loan Origination
  3. Gap analysis between evidence-derived and best practice
  4. BPMN for all of these so they can be modeled in Camunda

---

## LCD Terminology Note

In the transcript, David says "least competent nominator" which appears to be a speech-to-text error. In the PRD and all platform materials, this was interpreted as **"Least Common Denominator" (LCD)** - meaning the baseline/consensus process view synthesized from all available evidence. If David intended a different term, this should be clarified with him directly.
